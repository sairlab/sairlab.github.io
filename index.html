---
layout: default
title: SAIR lab
---

<section id="home">
<h1> 
	Social AI & Robotics (SAIR) Lab <br>
    
</h1>

<hr>

<table border="0" cellpadding="3" cellspacing="10">
	<tr>
	<td style="vertical-align:top">
		<img padding="0px 20px 15px 0px" src="images/profile.jpg"  width="400" height="inherit" border="1px" alt="">
	</td>
	<td>
		<b>Hello from SAIR! <b> <br>
        <b> We are part of <a href="https://www.kcl.ac.uk/research/core"> Centre for Robotics Research (CoRe)</a>, <a href="https://www.kcl.ac.uk/engineering"> Department of Engineering</a>, <a href="https://www.kcl.ac.uk"> King's College London</a>, United Kingdom. Broadly speaking, our research focuses on computer vision and machine learning for human-robot interaction. In particular, we are interested in learning multimodal representations of human behaviour and environment from data only and integrating such representations into the perception, learning and control of real-world systems such as robots. Key application areas include but not limited to autonomous systems and assistive technologies in healthcare, education, work, and interaction interfaces. <b><br>
        <b> <a href="mailto:sairlteam@gmail.com"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://twitter.com/SAIRLab"><i class="icon fab fa-twitter-square fa-2x"></i></a>
</a> <b>
    </td>
	</tr>
</table>

</section>

<section id="news">
    
    <h2> News </h2>
    <hr>
    <ul>
        <li> 05/01/2021 Call for papers on Frontiers Computer Science Special Topic “Multimodal Behavioural AI for Wellbeing”. More information is available <a href="https://www.frontiersin.org/research-topics/18176/multimodal-behavioural-ai-for-wellbeing">[here]</a>. </li>
        <li> 05/01/2021 Call for papers on MDPI Sensors Special Issue “Computer Vision Techniques Applied to Human Behaviour Analysis in the Real-World”. More information is available <a href="https://www.mdpi.com/journal/sensors/special_issues/human_behaviour_analysis">[here]</a>. </li>
        <li> 23/09/2020 We are looking for a postdoc to join us! For more information, contact <a href="mailto:oya.celiktutan@kcl.ac.uk">Oya </a> ! </li>
        <li> 12/10/2020 Dear prospective PhD students, if you are interested in joining us check out the funding opportunities at King's <a href="https://www.kcl.ac.uk/study/postgraduate/fees-and-funding/student-funding/postgraduate-research-funding">for international students </a> and <a href="https://www.kcl.ac.uk/study/funding/kings-china-scholarship-council-phd-scholarship-programme-k-csc"> for Chinese students only</a>  and contact <a href="mailto:oya.celiktutan@kcl.ac.uk">Oya </a> for further information! </li>
    
    </ul>
</section>


<section id="team">
<h2> Team </h2>

<hr>

<table border="0" cellpadding="3" cellspacing="10">
    <tr>
                <td><img  src="team/oya.png" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Oya Celiktutan, Principal Investigator </b> <br>
            <b>Assistant Professor / Lecturer in Robotics <br>
            <b> <a href="mailto:oya.celiktutan@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://nms.kcl.ac.uk/oya.celiktutan/"><i class="icon fa fa-globe-europe fa-2x"></i></a> <a href="https://scholar.google.es/citations?user=CCCoMqcAAAAJ"><i class="icon ai ai-google-scholar-square ai-2x"></i></a> <!--<a href="https://github.com/USERNAME"><i class="icon fab fa-github fa-2x"></i></a><br>--><br>
        </td>

       <td><img src="team/viktor.png" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
           
        <td>
            <b>Viktor Schmuck, PhD Student </b> <br>
            <b> Socially Aware Robotic Assistance <br>
            <b> <a href="mailto:viktor.schmuck@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://kclpure.kcl.ac.uk/portal/en/persons/viktor-schmuck(a408ac62-a1e7-410b-8340-45ae97c664bd).html"><i class="icon fa fa-globe-europe fa-2x"></i></a> <a href="https://scholar.google.es/citations?user=Nkaafj0AAAAJ"><i class="icon ai ai-google-scholar-square ai-2x"></i></a>  <a href="https://github.com/d4rkspir1t"><i class="icon fab fa-github fa-2x"></i></a><br>
        </td>
    </tr>
    
    <tr>
                <td><img  src="team/edo.jpg" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Edoardo Cetin, PhD Student </b> <br>
            <b> Deep Reinforcement Learning <br>
            <b> <a href="mailto:edoardo.cetin@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://kclpure.kcl.ac.uk/portal/en/persons/edoardo-cetin(59279b17-0276-4694-b74e-6d39236508b2).html"><i class="icon fa fa-globe-europe fa-2x"></i></a> <!--<a href="https://scholar.google.es/citations?user=USERID"><i class="icon ai ai-google-scholar-square ai-2x"></i></a>--> <!--<a href="https://github.com/USERNAME"><i class="icon fab fa-github fa-2x"></i></a><br>--><br>
        </td>

                <td><img  src="team/JJ.jpg" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px"</td>
        <td>
            <b>Jian Jiang, PhD Student </b> <br>
            <b> Continual Learning for Human-Robot Interaction <br>
            <b> <a href="mailto:jian.jiang@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://kclpure.kcl.ac.uk/portal/en/persons/jian-jiang(dbebff2c-7358-4169-88ed-9276c4172485).html"><i class="icon fa fa-globe-europe fa-2x"></i></a> <!--<a href="https://scholar.google.es/citations?user=USERID"><i class="icon ai ai-google-scholar-square ai-2x"></i></a>--> <!--<a href="https://github.com/USERNAME"><i class="icon fab fa-github fa-2x"></i></a><br>--><br>
        </td>
    </tr>

    <tr>
                <td><img  src="team/gerard.jpg" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Gerard Canal, Associate Member </b> <br>
            <b> Postdoctoral Researcher, Department of Informatics <br>
            <b> <a href="mailto:gerard.canal@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://gerardcanal.github.io"><i class="icon fa fa-globe-europe fa-2x"></i></a>  <a href="https://scholar.google.es/citations?user=5sioIgUAAAAJ"><i class="icon ai ai-google-scholar-square ai-2x"></i></a> <a href="https://github.com/gerardcanal"><i class="icon fab fa-github fa-2x"></i></a><br>
        </td>

                <td><img  src="team/miriam.jpg" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Miriam Redi, Visiting Research Fellow </b> <br>
            <b> Senior Research Scientist, Wikimedia Foundation <br>
            <b> <a href="mailto:miriam.redi@kcl.ac.uk"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="http://www.visionresearchwitch.com"><i class="icon fa fa-globe-europe fa-2x"></i></a> <a href="https://scholar.google.es/citations?user=s4oWIYgAAAAJ"><i class="icon ai ai-google-scholar-square ai-2x"></i></a> <!--<a href="https://github.com/USERNAME"><i class="icon fab fa-github fa-2x"></i></a><br>--><br>
        </td>
    </tr>
    
    <tr>
                <td><img  src="team/kaiko.png" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Kaiko, Human Support Robot </b> <br>
            <b> <br>
            <b> <a href="mailto:sairlteam@gmail.com"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://www.toyota-global.com/innovation/partner_robot/index.html"><i class="icon fa fa-globe-europe fa-2x"></i></a>  <br>
        </td>

                <td><img  src="team/emo.png" width="200" height="inherit" border="1px" style="border-radius:50%; vertical-align:middle; margin:10px" </td>
        <td>
            <b>Emo, NAO Robot </b> <br>
            <b>  <br>
            <b> <a href="mailto:sairlteam@gmail.com"><i class="icon fa fa-envelope fa-2x"></i></a> <a href="https://www.softbankrobotics.com/emea/en/nao"><i class="icon fa fa-globe-europe fa-2x"></i></a>  <br>
        </td>
    </tr>
    
</table>
</section>

<section id="publications">
    <h2> Publications </h2>
<hr>
    <table border="0" cellpadding="3" cellspacing="10">
        <tr>
            <td><a href="projects/Robocentric_CGD_ROMAN2020/index.html"><img style="vertical-align:middle" src="projects/Robocentric_CGD_ROMAN2020/thumb.jpg"  width="200" height="inherit" border="1px" alt="" /></a></td>
            <td>
                <b>Robocentric Conversational Group Discovery</b> <br>
                <b>Viktor Schmuck, Tingran Sheng, Oya Celiktutan<br>
                <em>Accepted to ROMAN2020: The 29th IEEE International Conference on Robot & Human Interactive Communication Proceedings, 2020</em> <br>
                [<a href="projects/Robocentric_CGD_ROMAN2020/index.html">Project page</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] -->
            </td>
        </tr>
        <tr>
            <td><a href="projects/RICA_UKRAS/index.html"><img style="vertical-align:middle" src="projects/RICA_UKRAS/thumb.jpg"  width="200" height="inherit" border="1px" alt="" /></a></td>
            <td>
                <b>RICA: Robocentric Indoor Crowd Analysis Dataset</b> <br>
                <b>Viktor Schmuck, Oya Celiktutan<br>
                <em>UKRAS20 Conference: “Robots into the real world” Proceedings, 2020</em> <br>
                [<a href="projects/RICA_UKRAS/index.html">Project page</a>] <!--  [<a href="projects/RICA_UKRAS/UKRAS20_paper_14.pdf">Paper</a>]-->
            </td>
        </tr>
        <tr>
            <td><a href="projects/IGTD2020/index.html"><img style="vertical-align:middle" src="projects/IGTD2020/teaser.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
            <td>
                <b>Inferring Student Engagement in Collaborative Problem Solving from Visual Cues</b> <br>
                <b>Angelika Kasparova, Oya Celiktutan, Mutlu Cukurova<br>
                <em>Companion Publication of the 2020 International Conference on Multimodal Interaction (ICMI '20 Companion)</em> <br>
                [<a href="projects/IGTD2020/index.html">Project page</a>]
            </td>
        </tr>
    </table>
</section>

<section id="projects">
    <h2> Projects </h2>
    <hr>
    
    <table border="0" cellpadding="3" cellspacing="10">
        <tr>
            <td><a href=""><img style="vertical-align:middle" src="projects/grants/lisi/lisi.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
            <td>
                <b>EPSRC New Investigator Award, 2021 - 2023 </b> <br>
                <b><em> LISI </em>  - <em>Learning</em> to <em>Imitate</em> Noverbal Communication Dynamics for Human-Robot <em>Social</em> <em>Interaction</em> <br>
                [<a href="projects/grants/lisi/index.html">Project page</a>]
            </td>

        </tr>

    </table>
    
</section>

<section id="partners">
    <h2> Collaborators </h2>
     <hr>
     
    <table border="0" cellpadding="3" cellspacing="10">
        <tr>
            <td><a href=""><img style="vertical-align:middle" src="images/tme.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
            <td>
                <b>Toyota Motor Europe </b> <br>

            </td>

        </tr>

    </table>
    

</section>


