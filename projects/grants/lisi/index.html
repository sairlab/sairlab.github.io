---
layout: default1
title: "Learning to Imitate Social Interaction"
---

<center><h1>LISI - Learning to Imitate Social Interaction</h1></center>

<td>
	<center>
                <img style="vertical-align:middle" src="lisi.png"  width="50%" height="inherit"/>
           <br>
		EPSRC New Investigator Award
        <br>
		<br>
        Dr Oya Celiktutan, <em> Principal Investigator </em><br>
		<br>
		<nobr> Centre for Robotics Research <nobr>
        <br>
        <nobr>    Department of Engineering </nobr>
        <br>
        <nobr>    King's College London </nobr>
        <br>
		<br>
        <nobr> <b> This position is still OPEN!
            <br> We are looking for a creative and motivated postdoctoral researcher to join us!
            <br> If interested, <a href="mailto:oya.celiktutan@kcl.ac.uk">[email Oya] </a> ! </b>  </nobr><br>

	</center>
</td>

<br>
	
<td>
    <hr>
    <h3 style="margin-bottom:10px;">Information for Prospective Candidates</h3>
    <b> How to apply: LINK WILL BE AVAILABLE SOON!    </b>
    <!-- please see the <a href="https://jobs.kcl.ac.uk/gb/en/job/008504/Research-Associate">[official application link] </a> -->
    <br><br>
    <b> Start date: </b> 1 March 2021, or soon after
    <br> 
    <b> Project duration: </b> 2 years / 24 months 
    <br><br>
    <b> About the Role </b>
    <br>
    An exciting Postdoctoral Research Associate position is available at the Department of Engineering, King’s College London to develop novel algorithms for learning the principles of human-human interaction autonomously from video data and integrating such models into the perception and control of humanoid robots. The main objective of the project is to set the basis for the next generation of robots that will be able to learn how to act in concert with humans by watching human-human interactions. The desired start date of the post is on the 1 March 2021, or soon after. <br>
    <br>
    The candidate will become a member of the Centre for Robotics Research (CORE), an excellent environment with state-of-the-art laboratories. CORE brings together specialised expertise in sensing, manipulation, machine learning, and intelligent control to develop world-leading solutions to society’s most critical challenges. Please see https://www.kcl.ac.uk/research/core.
    <br>
    <br>
    The candidate will contribute to ongoing research activities at the Social AI & Robotics laboratory (https://sairlab.github.io) in the areas of computer vision, machine learning and human-robot interaction and will be closely working with Dr Oya Celiktutan and her team, as well as her academic collaborators in the fields of natural language processing, education and psychology and the project partner SoftBank Robotics Europe. For more information, please see the summary below.
    <br><br>
    <b>About the Candidate</b>
    <br>
    The candidate should possess a PhD, or be near completion, in Engineering, Computer Science, or related field. They should have a strong research record in machine learning, with specialisation in one or more of the following areas: human-robot interaction, computer vision, deep learning, self-supervised/unsupervised learning, reinforcement learning, and/or human behaviour analysis and synthesis, as evidenced by publications in top-tier journals and/or conferences. <br>
    <br>
    The candidate should be motivated and creative about designing and implementing novel algorithms by taking into account the project’s aim. They should have excellent programming skills and should be competent to effectively integrate various systems such as robots and external sensors to run experiments. They should be willing to lead high-quality publications and work as part of a team. <br>
    
    <br>
    Key requirements include:
    <ul>
    <li> PhD degree (or shortly expect to receive) in Engineering, Computer Science, Robotics, or related field. </li>
    <li> Strong background in machine learning, with specialisation in one or more of the following areas: human-robot interaction, human-human interaction, computer vision, deep learning, self-supervised/unsupervised learning, reinforcement learning, and/or human behaviour analysis and synthesis. </li>
    <li> Excellent publication record in top-tier journals and/or conference proceedings, such as IEEE, ACM, CVF, RSJ, Springer, and Elsevier. </li>
    <li> Excellent programming skills, particularly, Python and/or C/C++. Hands-on experience with large-scale datasets, deep learning libraries (e.g., TensorFlow, PyTorch) and physical robots will be a plus.</li>
    <li> Excellent verbal and written communication skills. </li>
    <li> Excellent interpersonal / teamwork skills. </li>
    <li> Ability to organise and prioritise work to meet deadlines with minimal supervision.  </li>
    </ul>
    <br>
    
    In addition to completing the online application, candidates should attach:
    <ul>
    <li> CV with a full list of publications.</li>
    <li> Two-page summary of research background and how their experience and research plans fit with the post.  </li>
    <li> One of their best papers (either published or accepted for publication) demonstrating expertise in the areas mentioned above. </li>
    <li> The expected date of graduation (if currently pursuing a PhD). </li>
    <li> Contact details of two referees. </li>
    </ul>
    
</td>

<td>
    <hr>
    <h3 style="margin-bottom:10px;">Summary</h3>
    We are approaching a future where robots will progressively become widespread in many aspects of our daily lives, including education, healthcare, work and personal use. All of these practical applications require that humans and robots work together in human environments, where social interaction is unavoidable. Along with verbal communication, successful social interaction is closely coupled with the interplay between nonverbal perception and action mechanisms, such as observation of one’s gaze behaviour and following their attention, coordinating the form and function of hand-arm gestures. Humans perform social interaction in an instinctive and adaptive manner, with no effort. For robots to be successful in our social landscape, they should therefore engage in social interactions in a human-like manner, with increasing levels of autonomy.
    <br><br>
    Despite the exponential growth in the fields of human-robot interaction and social robotics, the capabilities of current social robots are still limited. First, most of the interaction contexts has been handled through tele-operation, whereby a human operator controls the robot remotely. However, this approach will be labour-intensive and impractical as the robots become more commonplace in our society. Second, designing interaction logic by manually programming each behaviour is exceptionally difficult, taking into account the complexity of the problem. Once fixed, it will be limited, not transferrable to unseen interaction contexts, and not robust to unpredicted inputs from the robot’s environment (e.g., sensor noise).
    <br><br>
    Data-driven approaches are a promising path for addressing these shortcomings as modelling human-human interaction is the most natural guide to designing human-robot interaction interfaces that can be usable and understandable by everyone. This project aims (1) to develop novel methods for learning the principles of human-human interaction autonomously from visual data and learning to imitate these principles via robots using the techniques of computer vision and machine learning, and (2) to synergistically integrate these methods into the perception and control of real humanoid robots. This project will set the basis for the next generation of robots that will be able to learn how to act in concert with humans by watching human-human interaction videos.
</td>


